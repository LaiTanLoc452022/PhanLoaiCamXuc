# train_model_fixed.py - Xá»­ lÃ½ nhÃ£n vÄƒn báº£n
import torch
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
import warnings
warnings.filterwarnings('ignore')
import os
import json

# Khai bÃ¡o cÃ¡c biáº¿n toÃ n cá»¥c
MODEL_NAME = "vinai/phobert-base-v2"
MAX_LENGTH = 256
BATCH_SIZE = 16
LEARNING_RATE = 2e-5
EPOCHS = 5

class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    
    def __len__(self):
        return len(self.labels)

def convert_labels_to_numeric(labels):
    """
    Chuyá»ƒn Ä‘á»•i nhÃ£n vÄƒn báº£n thÃ nh sá»‘
    """
    label_mapping = {
        'tiÃªu cá»±c': 0,
        'trung tÃ­nh': 1,
        'tÃ­ch cá»±c': 2,
        'negative': 0,
        'neutral': 1,
        'positive': 2
    }
    
    numeric_labels = []
    for label in labels:
        # Chuyá»ƒn vá» chá»¯ thÆ°á»ng vÃ  strip khoáº£ng tráº¯ng
        label_str = str(label).strip().lower()
        
        # Kiá»ƒm tra xem cÃ³ pháº£i lÃ  sá»‘ khÃ´ng
        if label_str.isdigit():
            label_int = int(label_str)
            if label_int in [0, 1, 2]:
                numeric_labels.append(label_int)
            else:
                # Náº¿u lÃ  sá»‘ nhÆ°ng khÃ´ng pháº£i 0,1,2
                print(f"âš ï¸  Cáº£nh bÃ¡o: NhÃ£n sá»‘ {label_int} khÃ´ng há»£p lá»‡, gÃ¡n máº·c Ä‘á»‹nh lÃ  trung tÃ­nh (1)")
                numeric_labels.append(1)
        else:
            # Náº¿u lÃ  vÄƒn báº£n, Ã¡nh xáº¡
            if label_str in label_mapping:
                numeric_labels.append(label_mapping[label_str])
            else:
                # NhÃ£n khÃ´ng xÃ¡c Ä‘á»‹nh, gÃ¡n máº·c Ä‘á»‹nh
                print(f"âš ï¸  Cáº£nh bÃ¡o: NhÃ£n '{label}' khÃ´ng xÃ¡c Ä‘á»‹nh, gÃ¡n máº·c Ä‘á»‹nh lÃ  trung tÃ­nh (1)")
                numeric_labels.append(1)
    
    return numeric_labels

def load_and_prepare_data(file_path="dataset.csv"):
    """
    Táº£i vÃ  chuáº©n bá»‹ dá»¯ liá»‡u tá»« file CSV
    """
    print("ğŸ“¥ Äang táº£i dá»¯ liá»‡u...")
    
    # Táº£i dá»¯ liá»‡u
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
    except:
        # Thá»­ encoding khÃ¡c náº¿u utf-8 khÃ´ng hoáº¡t Ä‘á»™ng
        df = pd.read_csv(file_path, encoding='latin1')
    
    # Kiá»ƒm tra cáº¥u trÃºc dá»¯ liá»‡u
    print(f"\nğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u:")
    print(f"  Sá»‘ hÃ ng: {len(df)}")
    print(f"  Sá»‘ cá»™t: {len(df.columns)}")
    print(f"  CÃ¡c cá»™t: {list(df.columns)}")
    
    # TÃ¬m cá»™t text vÃ  label
    text_column = None
    label_column = None
    
    # TÃ¬m cá»™t text (cÃ³ thá»ƒ cÃ³ tÃªn khÃ¡c)
    possible_text_columns = ['text', 'content', 'sentence', 'comment', 'review', 'vÄƒn báº£n', 'cÃ¢u']
    for col in df.columns:
        if col.lower() in possible_text_columns:
            text_column = col
            break
    
    # TÃ¬m cá»™t label (cÃ³ thá»ƒ cÃ³ tÃªn khÃ¡c)
    possible_label_columns = ['label', 'sentiment', 'emotion', 'category', 'nhÃ£n', 'cáº£m xÃºc']
    for col in df.columns:
        if col.lower() in possible_label_columns:
            label_column = col
            break
    
    if text_column is None:
        # Láº¥y cá»™t Ä‘áº§u tiÃªn lÃ m text
        text_column = df.columns[0]
    
    if label_column is None:
        # Láº¥y cá»™t thá»© hai lÃ m label (náº¿u cÃ³)
        if len(df.columns) > 1:
            label_column = df.columns[1]
        else:
            # Náº¿u chá»‰ cÃ³ má»™t cá»™t, táº¡o label máº·c Ä‘á»‹nh
            print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y cá»™t label, gÃ¡n máº·c Ä‘á»‹nh táº¥t cáº£ lÃ  trung tÃ­nh (1)")
            df['label'] = 1
            label_column = 'label'
    
    print(f"  Cá»™t text: {text_column}")
    print(f"  Cá»™t label: {label_column}")
    
    # LÃ m sáº¡ch dá»¯ liá»‡u
    df['text'] = df[text_column].astype(str).str.strip()
    df['label'] = df[label_column]
    
    # XÃ³a hÃ ng trá»‘ng
    df = df.dropna(subset=['text', 'label'])
    df = df[df['text'].str.strip() != '']
    
    # Chuyá»ƒn Ä‘á»•i nhÃ£n thÃ nh sá»‘
    print("\nğŸ”„ Äang chuyá»ƒn Ä‘á»•i nhÃ£n...")
    df['label_numeric'] = convert_labels_to_numeric(df['label'].tolist())
    
    # Kiá»ƒm tra phÃ¢n phá»‘i label
    print("\nğŸ“Š PhÃ¢n phá»‘i nhÃ£n gá»‘c:")
    original_dist = df[label_column].value_counts()
    for label, count in original_dist.items():
        print(f"  '{label}': {count} máº«u")
    
    print("\nğŸ“Š PhÃ¢n phá»‘i nhÃ£n sá»‘ hÃ³a:")
    numeric_dist = df['label_numeric'].value_counts().sort_index()
    label_names = {0: "TIÃŠU Cá»°C", 1: "TRUNG TÃNH", 2: "TÃCH Cá»°C"}
    for label_num, count in numeric_dist.items():
        label_name = label_names.get(label_num, f"KHÃ”NG XÃC Äá»ŠNH ({label_num})")
        print(f"  {label_name} ({label_num}): {count} máº«u ({count/len(df)*100:.1f}%)")
    
    # Chia dá»¯ liá»‡u
    print(f"\nğŸ“ˆ Chia dá»¯ liá»‡u...")
    train_df, temp_df = train_test_split(
        df, 
        test_size=0.3, 
        random_state=42, 
        stratify=df['label_numeric']
    )
    val_df, test_df = train_test_split(
        temp_df, 
        test_size=0.5, 
        random_state=42, 
        stratify=temp_df['label_numeric']
    )
    
    print(f"  Train: {len(train_df)} máº«u")
    print(f"  Validation: {len(val_df)} máº«u")
    print(f"  Test: {len(test_df)} máº«u")
    
    return train_df, val_df, test_df

def tokenize_data(tokenizer, train_df, val_df, test_df):
    """
    Tokenize dá»¯ liá»‡u
    """
    print("\nğŸ”¤ Äang tokenize dá»¯ liá»‡u...")
    
    # Láº¥y text vÃ  label
    train_texts = train_df['text'].tolist()
    train_labels = train_df['label_numeric'].tolist()
    
    val_texts = val_df['text'].tolist()
    val_labels = val_df['label_numeric'].tolist()
    
    test_texts = test_df['text'].tolist()
    test_labels = test_df['label_numeric'].tolist()
    
    # Tokenize
    train_encodings = tokenizer(
        train_texts,
        truncation=True,
        padding=True,
        max_length=MAX_LENGTH
    )
    
    val_encodings = tokenizer(
        val_texts,
        truncation=True,
        padding=True,
        max_length=MAX_LENGTH
    )
    
    test_encodings = tokenizer(
        test_texts,
        truncation=True,
        padding=True,
        max_length=MAX_LENGTH
    )
    
    # Táº¡o datasets
    train_dataset = SimpleDataset(train_encodings, train_labels)
    val_dataset = SimpleDataset(val_encodings, val_labels)
    test_dataset = SimpleDataset(test_encodings, test_labels)
    
    return train_dataset, val_dataset, test_dataset

def compute_metrics(p):
    """
    TÃ­nh toÃ¡n metrics cho evaluation
    """
    predictions, labels = p
    predictions = np.argmax(predictions, axis=1)
    
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    
    return {
        "accuracy": accuracy,
        "f1": f1
    }

def train_sentiment_model():
    """
    Huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n loáº¡i cáº£m xÃºc
    """
    print("ğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n loáº¡i cáº£m xÃºc tiáº¿ng Viá»‡t")
    print("=" * 60)
    
    # 1. Táº£i vÃ  chuáº©n bá»‹ dá»¯ liá»‡u
    try:
        train_df, val_df, test_df = load_and_prepare_data("dataset.csv")
    except FileNotFoundError:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file dataset.csv")
        print("ğŸ’¡ Táº¡o file dataset.csv vá»›i cáº¥u trÃºc:")
        print("   text,label")
        print("   'Sáº£n pháº©m ráº¥t tá»‘t',tÃ­ch cá»±c")
        print("   'Dá»‹ch vá»¥ tá»‡',tiÃªu cá»±c")
        print("   'BÃ¬nh thÆ°á»ng',trung tÃ­nh")
        return
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 2. Táº£i tokenizer
    print("\nğŸ”„ Äang táº£i tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # 3. Tokenize dá»¯ liá»‡u
    train_dataset, val_dataset, test_dataset = tokenize_data(tokenizer, train_df, val_df, test_df)
    
    # 4. Táº¡o data collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # 5. Táº£i mÃ´ hÃ¬nh
    print("\nğŸ§  Äang táº£i mÃ´ hÃ¬nh PhoBERT...")
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=3,
        id2label={0: "TIÃŠU Cá»°C", 1: "TRUNG TÃNH", 2: "TÃCH Cá»°C"},
        label2id={"TIÃŠU Cá»°C": 0, "TRUNG TÃNH": 1, "TÃCH Cá»°C": 2},
        ignore_mismatched_sizes=True
    )
    
    # 6. Cáº¥u hÃ¬nh training
    training_args = TrainingArguments(
        output_dir="./sentiment_model",
        overwrite_output_dir=True,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        weight_decay=0.01,
        warmup_ratio=0.1,
        logging_dir="./logs",
        logging_steps=50,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        report_to="none",
        fp16=torch.cuda.is_available(),
        push_to_hub=False,
    )
    
    # 7. Táº¡o Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    
    # 8. Huáº¥n luyá»‡n
    print("\nğŸ‹ï¸â€â™‚ï¸ Báº¯t Ä‘áº§u huáº¥n luyá»‡n...")
    train_result = trainer.train()
    
    # 9. LÆ°u mÃ´ hÃ¬nh
    print("\nğŸ’¾ Äang lÆ°u mÃ´ hÃ¬nh...")
    trainer.save_model("./sentiment_model")
    tokenizer.save_pretrained("./sentiment_model")
    
    # 10. ÄÃ¡nh giÃ¡ trÃªn táº­p test
    print("\nğŸ“Š Äang Ä‘Ã¡nh giÃ¡ trÃªn táº­p test...")
    test_results = trainer.evaluate(test_dataset)
    
    print("\n" + "=" * 60)
    print("âœ… HUáº¤N LUYá»†N HOÃ€N Táº¤T!")
    print("=" * 60)
    
    print("\nğŸ“ˆ Káº¿t quáº£ huáº¥n luyá»‡n:")
    print(f"  Train loss: {train_result.training_loss:.4f}")
    print(f"  Test accuracy: {test_results.get('eval_accuracy', 0):.4f}")
    print(f"  Test F1-score: {test_results.get('eval_f1', 0):.4f}")
    
    # 11. Dá»± Ä‘oÃ¡n trÃªn táº­p test
    print("\nğŸ“‹ BÃ¡o cÃ¡o phÃ¢n loáº¡i chi tiáº¿t:")
    test_predictions = trainer.predict(test_dataset)
    y_pred = np.argmax(test_predictions.predictions, axis=-1)
    y_true = test_predictions.label_ids
    
    # Táº¡o classification report
    target_names = ["TIÃŠU Cá»°C", "TRUNG TÃNH", "TÃCH Cá»°C"]
    report = classification_report(y_true, y_pred, target_names=target_names, digits=4)
    print(report)
    
    # 12. LÆ°u thÃ´ng tin training
    training_info = {
        "model_name": MODEL_NAME,
        "training_date": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
        "num_train_samples": len(train_df),
        "num_val_samples": len(val_df),
        "num_test_samples": len(test_df),
        "max_length": MAX_LENGTH,
        "batch_size": BATCH_SIZE,
        "learning_rate": LEARNING_RATE,
        "epochs": EPOCHS,
        "test_accuracy": float(test_results.get('eval_accuracy', 0)),
        "test_f1": float(test_results.get('eval_f1', 0)),
        "label_mapping": {
            "0": "TIÃŠU Cá»°C",
            "1": "TRUNG TÃNH", 
            "2": "TÃCH Cá»°C"
        }
    }
    
    # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i
    os.makedirs("./sentiment_model", exist_ok=True)
    
    with open("./sentiment_model/training_info.json", "w", encoding="utf-8") as f:
        json.dump(training_info, f, ensure_ascii=False, indent=2)
    
    print("\nğŸ“ MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: ./sentiment_model/")
    
    # 13. Test vá»›i má»™t sá»‘ cÃ¢u máº«u
    print("\nğŸ§ª Test vá»›i cÃ¢u máº«u:")
    test_sentences = [
        "Sáº£n pháº©m nÃ y ráº¥t tá»‘t, tÃ´i ráº¥t hÃ i lÃ²ng",
        "Dá»‹ch vá»¥ tá»‡ quÃ¡, khÃ´ng bao giá» quay láº¡i",
        "CÅ©ng bÃ¬nh thÆ°á»ng, khÃ´ng cÃ³ gÃ¬ Ä‘áº·c biá»‡t"
    ]
    
    for sentence in test_sentences:
        inputs = tokenizer(sentence, return_tensors="pt", truncation=True, max_length=MAX_LENGTH)
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        predicted_label = torch.argmax(predictions).item()
        label_name = {0: "TIÃŠU Cá»°C", 1: "TRUNG TÃNH", 2: "TÃCH Cá»°C"}.get(predicted_label, "UNKNOWN")
        confidence = torch.max(predictions).item()
        
        print(f"  '{sentence}'")
        print(f"    â†’ {label_name} ({confidence:.2%})")
    
    print("\nğŸ¯ Äá»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh trong á»©ng dá»¥ng chÃ­nh:")
    print("   Thay Ä‘á»•i trong main.py:")
    print("   model_path = './sentiment_model'")
    print("   model = AutoModelForSequenceClassification.from_pretrained(model_path)")
    
    return trainer, test_results

if __name__ == "__main__":
    # Kiá»ƒm tra GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  Thiáº¿t bá»‹ Ä‘ang sá»­ dá»¥ng: {device}")
    if torch.cuda.is_available():
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ PhiÃªn báº£n PyTorch: {torch.__version__}")
    
    # Huáº¥n luyá»‡n mÃ´ hÃ¬nh
    train_sentiment_model()